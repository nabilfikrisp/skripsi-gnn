{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "# Set the seed for general torch operations\n",
    "torch.manual_seed(42)\n",
    "# Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "torch.cuda.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 8\n",
    "LEARNING_RATE = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/train'), WindowsPath('data/test'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = Path(\"./data/train/\")\n",
    "test_dir = Path(\"./data/test/\")\n",
    "\n",
    "train_dir, test_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "manual_transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.Resize((224, 224)),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset ImageFolder\n",
       "     Number of datapoints: 1250\n",
       "     Root location: data\\train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                  ToImage()\n",
       "                  Resize(size=[224, 224], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
       "                  ToDtype(scale=True)\n",
       "                  Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
       "            ),\n",
       " Dataset ImageFolder\n",
       "     Number of datapoints: 500\n",
       "     Root location: data\\test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                  ToImage()\n",
       "                  Resize(size=[224, 224], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
       "                  ToDtype(scale=True)\n",
       "                  Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
       "            ),\n",
       " ['downdog', 'goddess', 'plank', 'tree', 'warrior2'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "# Use ImageFolder to create dataset(s)\n",
    "train_data = datasets.ImageFolder(str(train_dir), transform=manual_transforms)\n",
    "test_data = datasets.ImageFolder(str(test_dir), transform=manual_transforms)\n",
    "\n",
    "class_names = test_data.classes\n",
    "\n",
    "train_data, test_data, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x20206122bc0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x20206122da0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # don't need to shuffle test data\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_vig import vig_ti_224_gelu\n",
    "from model_mobile_vig import mobilevig_ti\n",
    "\n",
    "model = mobilevig_ti(num_classes=len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = torch.ones(1, dtype=torch.long)\n",
    "# target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model_vig import vig_ti_224_gelu\n",
    "# from model_mobile_vig import mobilevig_ti\n",
    "\n",
    "# mobile_model = mobilevig_ti(num_classes=len(class_names))\n",
    "# vig_model = vig_ti_224_gelu(num_classes=len(class_names))\n",
    "\n",
    "# img = torch.randn(1, 3, 224, 224)\n",
    "# mobile_model.eval()\n",
    "# vig_model.eval()\n",
    "# with torch.inference_mode():\n",
    "#     mobile_pred = mobile_model(img)\n",
    "#     vig_pred = vig_model(img)\n",
    "\n",
    "# print(f\"mobile {mobile_pred.shape}\")\n",
    "# print(f\"vig {vig_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# vig_loss = loss_fn(vig_pred, target)  # Assuming `target` is your ground truth labels\n",
    "# mobile_loss = loss_fn(\n",
    "#     mobile_pred, target\n",
    "# )  # Assuming `target` is your ground truth labels\n",
    "\n",
    "# print(f\"vig loss: {vig_loss.item()}\")\n",
    "# print(f\"mobile loss: {mobile_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "MobileViG (MobileViG)                              [128, 3, 224, 224]   [128, 5]             --                   True\n",
       "├─Stem (stem)                                      [128, 3, 224, 224]   [128, 42, 56, 56]    --                   True\n",
       "│    └─Sequential (stem)                           [128, 3, 224, 224]   [128, 42, 56, 56]    --                   True\n",
       "│    │    └─Conv2d (0)                             [128, 3, 224, 224]   [128, 21, 112, 112]  588                  True\n",
       "│    │    └─BatchNorm2d (1)                        [128, 21, 112, 112]  [128, 21, 112, 112]  42                   True\n",
       "│    │    └─GELU (2)                               [128, 21, 112, 112]  [128, 21, 112, 112]  --                   --\n",
       "│    │    └─Conv2d (3)                             [128, 21, 112, 112]  [128, 42, 56, 56]    7,980                True\n",
       "│    │    └─BatchNorm2d (4)                        [128, 42, 56, 56]    [128, 42, 56, 56]    84                   True\n",
       "│    │    └─GELU (5)                               [128, 42, 56, 56]    [128, 42, 56, 56]    --                   --\n",
       "├─ModuleList (local_backbone)                      --                   --                   --                   True\n",
       "│    └─InvertedResidual (0)                        [128, 42, 56, 56]    [128, 42, 56, 56]    42                   True\n",
       "│    │    └─MLP (mlp)                              [128, 42, 56, 56]    [128, 42, 56, 56]    16,758               True\n",
       "│    │    └─Identity (drop_path)                   [128, 42, 56, 56]    [128, 42, 56, 56]    --                   --\n",
       "│    └─InvertedResidual (1)                        [128, 42, 56, 56]    [128, 42, 56, 56]    42                   True\n",
       "│    │    └─MLP (mlp)                              [128, 42, 56, 56]    [128, 42, 56, 56]    16,758               True\n",
       "│    │    └─DropPath (drop_path)                   [128, 42, 56, 56]    [128, 42, 56, 56]    --                   --\n",
       "│    └─Downsample (2)                              [128, 42, 56, 56]    [128, 84, 28, 28]    --                   True\n",
       "│    │    └─Sequential (conv)                      [128, 42, 56, 56]    [128, 84, 28, 28]    32,004               True\n",
       "│    └─InvertedResidual (3)                        [128, 84, 28, 28]    [128, 84, 28, 28]    84                   True\n",
       "│    │    └─MLP (mlp)                              [128, 84, 28, 28]    [128, 84, 28, 28]    61,740               True\n",
       "│    │    └─DropPath (drop_path)                   [128, 84, 28, 28]    [128, 84, 28, 28]    --                   --\n",
       "│    └─InvertedResidual (4)                        [128, 84, 28, 28]    [128, 84, 28, 28]    84                   True\n",
       "│    │    └─MLP (mlp)                              [128, 84, 28, 28]    [128, 84, 28, 28]    61,740               True\n",
       "│    │    └─DropPath (drop_path)                   [128, 84, 28, 28]    [128, 84, 28, 28]    --                   --\n",
       "│    └─Downsample (5)                              [128, 84, 28, 28]    [128, 168, 14, 14]   --                   True\n",
       "│    │    └─Sequential (conv)                      [128, 84, 28, 28]    [128, 168, 14, 14]   127,512              True\n",
       "│    └─InvertedResidual (6)                        [128, 168, 14, 14]   [128, 168, 14, 14]   168                  True\n",
       "│    │    └─MLP (mlp)                              [128, 168, 14, 14]   [128, 168, 14, 14]   236,376              True\n",
       "│    │    └─DropPath (drop_path)                   [128, 168, 14, 14]   [128, 168, 14, 14]   --                   --\n",
       "│    └─InvertedResidual (7)                        [128, 168, 14, 14]   [128, 168, 14, 14]   168                  True\n",
       "│    │    └─MLP (mlp)                              [128, 168, 14, 14]   [128, 168, 14, 14]   236,376              True\n",
       "│    │    └─DropPath (drop_path)                   [128, 168, 14, 14]   [128, 168, 14, 14]   --                   --\n",
       "│    └─InvertedResidual (8)                        [128, 168, 14, 14]   [128, 168, 14, 14]   168                  True\n",
       "│    │    └─MLP (mlp)                              [128, 168, 14, 14]   [128, 168, 14, 14]   236,376              True\n",
       "│    │    └─DropPath (drop_path)                   [128, 168, 14, 14]   [128, 168, 14, 14]   --                   --\n",
       "│    └─InvertedResidual (9)                        [128, 168, 14, 14]   [128, 168, 14, 14]   168                  True\n",
       "│    │    └─MLP (mlp)                              [128, 168, 14, 14]   [128, 168, 14, 14]   236,376              True\n",
       "│    │    └─DropPath (drop_path)                   [128, 168, 14, 14]   [128, 168, 14, 14]   --                   --\n",
       "│    └─InvertedResidual (10)                       [128, 168, 14, 14]   [128, 168, 14, 14]   168                  True\n",
       "│    │    └─MLP (mlp)                              [128, 168, 14, 14]   [128, 168, 14, 14]   236,376              True\n",
       "│    │    └─DropPath (drop_path)                   [128, 168, 14, 14]   [128, 168, 14, 14]   --                   --\n",
       "│    └─InvertedResidual (11)                       [128, 168, 14, 14]   [128, 168, 14, 14]   168                  True\n",
       "│    │    └─MLP (mlp)                              [128, 168, 14, 14]   [128, 168, 14, 14]   236,376              True\n",
       "│    │    └─DropPath (drop_path)                   [128, 168, 14, 14]   [128, 168, 14, 14]   --                   --\n",
       "│    └─Downsample (12)                             [128, 168, 14, 14]   [128, 256, 7, 7]     --                   True\n",
       "│    │    └─Sequential (conv)                      [128, 168, 14, 14]   [128, 256, 7, 7]     387,840              True\n",
       "├─ModuleList (backbone)                            --                   --                   --                   True\n",
       "│    └─Sequential (0)                              [128, 256, 7, 7]     [128, 256, 7, 7]     --                   True\n",
       "│    │    └─Grapher (0)                            [128, 256, 7, 7]     [128, 256, 7, 7]     461,824              True\n",
       "│    │    └─FFN (1)                                [128, 256, 7, 7]     [128, 256, 7, 7]     528,128              True\n",
       "│    └─Sequential (1)                              [128, 256, 7, 7]     [128, 256, 7, 7]     --                   True\n",
       "│    │    └─Grapher (0)                            [128, 256, 7, 7]     [128, 256, 7, 7]     461,824              True\n",
       "│    │    └─FFN (1)                                [128, 256, 7, 7]     [128, 256, 7, 7]     528,128              True\n",
       "├─Sequential (prediction)                          [128, 256, 7, 7]     [128, 512, 1, 1]     --                   True\n",
       "│    └─AdaptiveAvgPool2d (0)                       [128, 256, 7, 7]     [128, 256, 1, 1]     --                   --\n",
       "│    └─Conv2d (1)                                  [128, 256, 1, 1]     [128, 512, 1, 1]     131,584              True\n",
       "│    └─BatchNorm2d (2)                             [128, 512, 1, 1]     [128, 512, 1, 1]     1,024                True\n",
       "│    └─GELU (3)                                    [128, 512, 1, 1]     [128, 512, 1, 1]     --                   --\n",
       "│    └─Dropout (4)                                 [128, 512, 1, 1]     [128, 512, 1, 1]     --                   --\n",
       "├─Conv2d (head)                                    [128, 512, 1, 1]     [128, 5, 1, 1]       2,565                True\n",
       "==================================================================================================================================\n",
       "Total params: 4,247,639\n",
       "Trainable params: 4,247,639\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 85.40\n",
       "==================================================================================================================================\n",
       "Input size (MB): 77.07\n",
       "Forward/backward pass size (MB): 12425.43\n",
       "Params size (MB): 16.99\n",
       "Estimated Total Size (MB): 12519.49\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(\n",
    "    model=model,\n",
    "    input_size=(\n",
    "        BATCH_SIZE,\n",
    "        3,\n",
    "        224,\n",
    "        224,\n",
    "    ),  # make sure this is \"input_size\", not \"input_shape\"\n",
    "    # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72533877344e400da8f68e38e614e603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60dfcf6ddce4800abc4218c7eae1fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train loss: 1.85696 | Test loss: 5.85790 | Train acc: 24.28% | Test acc: 21.73%\n",
      "Epoch: 2 | Train loss: 1.53297 | Test loss: 1.76362 | Train acc: 35.09% | Test acc: 36.22%\n",
      "Epoch: 3 | Train loss: 1.31567 | Test loss: 1.38042 | Train acc: 48.41% | Test acc: 45.36%\n",
      "Epoch: 4 | Train loss: 1.08317 | Test loss: 1.35707 | Train acc: 57.45% | Test acc: 42.82%\n",
      "Epoch: 5 | Train loss: 0.83059 | Test loss: 1.08740 | Train acc: 69.19% | Test acc: 57.85%\n",
      "Epoch: 6 | Train loss: 0.54105 | Test loss: 0.98681 | Train acc: 82.54% | Test acc: 65.36%\n",
      "Epoch: 7 | Train loss: 0.38272 | Test loss: 0.96972 | Train acc: 87.40% | Test acc: 68.28%\n",
      "Epoch: 8 | Train loss: 0.25722 | Test loss: 1.24421 | Train acc: 91.61% | Test acc: 62.39%\n",
      "Epoch: 9 | Train loss: 0.18873 | Test loss: 1.45020 | Train acc: 93.71% | Test acc: 61.61%\n",
      "Epoch: 10 | Train loss: 0.21613 | Test loss: 1.43805 | Train acc: 93.65% | Test acc: 63.62%\n",
      "[INFO] Total training time: 1556.623 seconds\n"
     ]
    }
   ],
   "source": [
    "import engine\n",
    "import torch.nn as nn\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Start the timer\n",
    "start_time = timer()\n",
    "\n",
    "# Setup training and save the results\n",
    "results = engine.train(\n",
    "    model=model.to(device),\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    device=torch.device(device),\n",
    "    use_progress_bar=True,\n",
    ")\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
